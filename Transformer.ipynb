{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義 Transforme rModel\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, num_heads):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, 1, batch_first=True)\n",
    "        self.transformer = Transformer(\n",
    "            d_model=hidden_dim,\n",
    "            #d_model=input_dim,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        #self.fc = nn.Linear(BATCH_SIZE*input_dim, output_dim)\n",
    "\n",
    "    def forward(self, src):\n",
    "    \n",
    "        # batch_size, _ = src.shape[0], src.shape[1]\n",
    "        # h_0 = torch.randn(1, batch_size,  self.hidden_dim).to(device)\n",
    "        # c_0 = torch.randn(1, batch_size,  self.hidden_dim).to(device)\n",
    "        # src, _ = self.lstm(src, (h_0, c_0))\n",
    "        src = self.embedding(src)\n",
    "        # #print('lstm後的大小:',src.shape)\n",
    "        output = self.transformer(src, src)\n",
    "        #print('T後的大小:',output.shape)\n",
    "        output = self.fc(output)\n",
    "        #output = self.fc(output)\n",
    "        #print('最終大小:',output.shape)\n",
    "        output = output[:, -1, :]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型參數\n",
    "input_dim = 10 # 輸入詞彙表大小\n",
    "hidden_dim = 32  # 隱藏層維度\n",
    "output_dim = 1  # 輸出維度，輸出維度為1，代表預測的下一個數\n",
    "num_layers = 4  # Transformer Encoder/Decoder 層數\n",
    "num_heads = 8  # Attention heads \n",
    "\n",
    "\n",
    "\n",
    "# 創建 Transformer\n",
    "model = TransformerModel(input_dim, hidden_dim, output_dim, num_layers, num_heads)\n",
    "model.to(device)\n",
    "\n",
    "# 定義損失函數和優化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(final_seq, batch_size):\n",
    "    \"\"\"\n",
    "    Batches the sequence data using PyTorch DataLoader.\n",
    "\n",
    "    Args:\n",
    "        final_seq: A list of tuples containing sequence input data and target data.\n",
    "        batch_size: The desired batch size.\n",
    "\n",
    "    Returns:\n",
    "        A DataLoader object containing the batched sequence data.\n",
    "\n",
    "    \"\"\"\n",
    "    final_seq = DataLoader(dataset=final_seq, batch_size=batch_size, shuffle=False, num_workers=0, drop_last=False)\n",
    "\n",
    "    return final_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成 l * w * h 維度的資料\n",
    "# l : batch_size\n",
    "# w : sequence length\n",
    "# h : amounts of features \n",
    "# 以下以 30 * 6 * 1 做示範\n",
    "def data_generated(beg, num, max=1):\n",
    "    final_seq = []\n",
    "    for i in range(num):\n",
    "        end = beg + 30\n",
    "        q_tensors = [torch.tensor([i/max]*10, dtype=torch.float32) for i in range(beg, end)]\n",
    "        q_tensors = torch.stack(q_tensors)\n",
    "        a_tensors = torch.tensor([end/max], dtype=torch.float32) \n",
    "        final_seq.append((q_tensors, a_tensors))\n",
    "        beg += 1\n",
    "    print('final_seq : 資料類型={}, 列數={}'.format(type(final_seq), len(final_seq)))\n",
    "    print('final_seq[0] : 資料類型={}, 列數={}'.format(type(final_seq[0]), len(final_seq[0])))\n",
    "    print('final_seq[0][0] : 資料類型={}, 內容數={}'.format(type(final_seq[0][0]), final_seq[0][0].shape))\n",
    "    print('final_seq[0][1] : 資料類型={}, 內容數={}'.format(type(final_seq[0][1]), final_seq[0][1].shape))\n",
    "    print(final_seq[0][0])\n",
    "    print(final_seq[0][1])\n",
    "    data = batch_data(final_seq, 30)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_seq : 資料類型=<class 'list'>, 列數=571\n",
      "final_seq[0] : 資料類型=<class 'tuple'>, 列數=2\n",
      "final_seq[0][0] : 資料類型=<class 'torch.Tensor'>, 內容數=torch.Size([30, 10])\n",
      "final_seq[0][1] : 資料類型=<class 'torch.Tensor'>, 內容數=torch.Size([1])\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "         0.0010],\n",
      "        [0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n",
      "         0.0020],\n",
      "        [0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n",
      "         0.0030],\n",
      "        [0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "         0.0040],\n",
      "        [0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
      "         0.0050],\n",
      "        [0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060,\n",
      "         0.0060],\n",
      "        [0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070,\n",
      "         0.0070],\n",
      "        [0.0080, 0.0080, 0.0080, 0.0080, 0.0080, 0.0080, 0.0080, 0.0080, 0.0080,\n",
      "         0.0080],\n",
      "        [0.0090, 0.0090, 0.0090, 0.0090, 0.0090, 0.0090, 0.0090, 0.0090, 0.0090,\n",
      "         0.0090],\n",
      "        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
      "         0.0100],\n",
      "        [0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
      "         0.0110],\n",
      "        [0.0120, 0.0120, 0.0120, 0.0120, 0.0120, 0.0120, 0.0120, 0.0120, 0.0120,\n",
      "         0.0120],\n",
      "        [0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
      "         0.0130],\n",
      "        [0.0140, 0.0140, 0.0140, 0.0140, 0.0140, 0.0140, 0.0140, 0.0140, 0.0140,\n",
      "         0.0140],\n",
      "        [0.0150, 0.0150, 0.0150, 0.0150, 0.0150, 0.0150, 0.0150, 0.0150, 0.0150,\n",
      "         0.0150],\n",
      "        [0.0160, 0.0160, 0.0160, 0.0160, 0.0160, 0.0160, 0.0160, 0.0160, 0.0160,\n",
      "         0.0160],\n",
      "        [0.0170, 0.0170, 0.0170, 0.0170, 0.0170, 0.0170, 0.0170, 0.0170, 0.0170,\n",
      "         0.0170],\n",
      "        [0.0180, 0.0180, 0.0180, 0.0180, 0.0180, 0.0180, 0.0180, 0.0180, 0.0180,\n",
      "         0.0180],\n",
      "        [0.0190, 0.0190, 0.0190, 0.0190, 0.0190, 0.0190, 0.0190, 0.0190, 0.0190,\n",
      "         0.0190],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200,\n",
      "         0.0200],\n",
      "        [0.0210, 0.0210, 0.0210, 0.0210, 0.0210, 0.0210, 0.0210, 0.0210, 0.0210,\n",
      "         0.0210],\n",
      "        [0.0220, 0.0220, 0.0220, 0.0220, 0.0220, 0.0220, 0.0220, 0.0220, 0.0220,\n",
      "         0.0220],\n",
      "        [0.0230, 0.0230, 0.0230, 0.0230, 0.0230, 0.0230, 0.0230, 0.0230, 0.0230,\n",
      "         0.0230],\n",
      "        [0.0240, 0.0240, 0.0240, 0.0240, 0.0240, 0.0240, 0.0240, 0.0240, 0.0240,\n",
      "         0.0240],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "         0.0250],\n",
      "        [0.0260, 0.0260, 0.0260, 0.0260, 0.0260, 0.0260, 0.0260, 0.0260, 0.0260,\n",
      "         0.0260],\n",
      "        [0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270,\n",
      "         0.0270],\n",
      "        [0.0280, 0.0280, 0.0280, 0.0280, 0.0280, 0.0280, 0.0280, 0.0280, 0.0280,\n",
      "         0.0280],\n",
      "        [0.0290, 0.0290, 0.0290, 0.0290, 0.0290, 0.0290, 0.0290, 0.0290, 0.0290,\n",
      "         0.0290]])\n",
      "tensor([0.0300])\n"
     ]
    }
   ],
   "source": [
    "data = data_generated(0, 571,1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_seq : 資料類型=<class 'list'>, 列數=970\n",
      "final_seq[0] : 資料類型=<class 'tuple'>, 列數=2\n",
      "final_seq[0][0] : 資料類型=<class 'torch.Tensor'>, 內容數=torch.Size([30, 10])\n",
      "final_seq[0][1] : 資料類型=<class 'torch.Tensor'>, 內容數=torch.Size([1])\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "         0.0010],\n",
      "        [0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n",
      "         0.0020],\n",
      "        [0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n",
      "         0.0030],\n",
      "        [0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "         0.0040],\n",
      "        [0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
      "         0.0050],\n",
      "        [0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060,\n",
      "         0.0060],\n",
      "        [0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070,\n",
      "         0.0070],\n",
      "        [0.0080, 0.0080, 0.0080, 0.0080, 0.0080, 0.0080, 0.0080, 0.0080, 0.0080,\n",
      "         0.0080],\n",
      "        [0.0090, 0.0090, 0.0090, 0.0090, 0.0090, 0.0090, 0.0090, 0.0090, 0.0090,\n",
      "         0.0090],\n",
      "        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
      "         0.0100],\n",
      "        [0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
      "         0.0110],\n",
      "        [0.0120, 0.0120, 0.0120, 0.0120, 0.0120, 0.0120, 0.0120, 0.0120, 0.0120,\n",
      "         0.0120],\n",
      "        [0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
      "         0.0130],\n",
      "        [0.0140, 0.0140, 0.0140, 0.0140, 0.0140, 0.0140, 0.0140, 0.0140, 0.0140,\n",
      "         0.0140],\n",
      "        [0.0150, 0.0150, 0.0150, 0.0150, 0.0150, 0.0150, 0.0150, 0.0150, 0.0150,\n",
      "         0.0150],\n",
      "        [0.0160, 0.0160, 0.0160, 0.0160, 0.0160, 0.0160, 0.0160, 0.0160, 0.0160,\n",
      "         0.0160],\n",
      "        [0.0170, 0.0170, 0.0170, 0.0170, 0.0170, 0.0170, 0.0170, 0.0170, 0.0170,\n",
      "         0.0170],\n",
      "        [0.0180, 0.0180, 0.0180, 0.0180, 0.0180, 0.0180, 0.0180, 0.0180, 0.0180,\n",
      "         0.0180],\n",
      "        [0.0190, 0.0190, 0.0190, 0.0190, 0.0190, 0.0190, 0.0190, 0.0190, 0.0190,\n",
      "         0.0190],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200,\n",
      "         0.0200],\n",
      "        [0.0210, 0.0210, 0.0210, 0.0210, 0.0210, 0.0210, 0.0210, 0.0210, 0.0210,\n",
      "         0.0210],\n",
      "        [0.0220, 0.0220, 0.0220, 0.0220, 0.0220, 0.0220, 0.0220, 0.0220, 0.0220,\n",
      "         0.0220],\n",
      "        [0.0230, 0.0230, 0.0230, 0.0230, 0.0230, 0.0230, 0.0230, 0.0230, 0.0230,\n",
      "         0.0230],\n",
      "        [0.0240, 0.0240, 0.0240, 0.0240, 0.0240, 0.0240, 0.0240, 0.0240, 0.0240,\n",
      "         0.0240],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "         0.0250],\n",
      "        [0.0260, 0.0260, 0.0260, 0.0260, 0.0260, 0.0260, 0.0260, 0.0260, 0.0260,\n",
      "         0.0260],\n",
      "        [0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270,\n",
      "         0.0270],\n",
      "        [0.0280, 0.0280, 0.0280, 0.0280, 0.0280, 0.0280, 0.0280, 0.0280, 0.0280,\n",
      "         0.0280],\n",
      "        [0.0290, 0.0290, 0.0290, 0.0290, 0.0290, 0.0290, 0.0290, 0.0290, 0.0290,\n",
      "         0.0290]])\n",
      "tensor([0.0300])\n",
      "final_seq : 資料類型=<class 'list'>, 列數=60\n",
      "final_seq[0] : 資料類型=<class 'tuple'>, 列數=2\n",
      "final_seq[0][0] : 資料類型=<class 'torch.Tensor'>, 內容數=torch.Size([30, 10])\n",
      "final_seq[0][1] : 資料類型=<class 'torch.Tensor'>, 內容數=torch.Size([1])\n",
      "tensor([[0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "         0.6000],\n",
      "        [0.6010, 0.6010, 0.6010, 0.6010, 0.6010, 0.6010, 0.6010, 0.6010, 0.6010,\n",
      "         0.6010],\n",
      "        [0.6020, 0.6020, 0.6020, 0.6020, 0.6020, 0.6020, 0.6020, 0.6020, 0.6020,\n",
      "         0.6020],\n",
      "        [0.6030, 0.6030, 0.6030, 0.6030, 0.6030, 0.6030, 0.6030, 0.6030, 0.6030,\n",
      "         0.6030],\n",
      "        [0.6040, 0.6040, 0.6040, 0.6040, 0.6040, 0.6040, 0.6040, 0.6040, 0.6040,\n",
      "         0.6040],\n",
      "        [0.6050, 0.6050, 0.6050, 0.6050, 0.6050, 0.6050, 0.6050, 0.6050, 0.6050,\n",
      "         0.6050],\n",
      "        [0.6060, 0.6060, 0.6060, 0.6060, 0.6060, 0.6060, 0.6060, 0.6060, 0.6060,\n",
      "         0.6060],\n",
      "        [0.6070, 0.6070, 0.6070, 0.6070, 0.6070, 0.6070, 0.6070, 0.6070, 0.6070,\n",
      "         0.6070],\n",
      "        [0.6080, 0.6080, 0.6080, 0.6080, 0.6080, 0.6080, 0.6080, 0.6080, 0.6080,\n",
      "         0.6080],\n",
      "        [0.6090, 0.6090, 0.6090, 0.6090, 0.6090, 0.6090, 0.6090, 0.6090, 0.6090,\n",
      "         0.6090],\n",
      "        [0.6100, 0.6100, 0.6100, 0.6100, 0.6100, 0.6100, 0.6100, 0.6100, 0.6100,\n",
      "         0.6100],\n",
      "        [0.6110, 0.6110, 0.6110, 0.6110, 0.6110, 0.6110, 0.6110, 0.6110, 0.6110,\n",
      "         0.6110],\n",
      "        [0.6120, 0.6120, 0.6120, 0.6120, 0.6120, 0.6120, 0.6120, 0.6120, 0.6120,\n",
      "         0.6120],\n",
      "        [0.6130, 0.6130, 0.6130, 0.6130, 0.6130, 0.6130, 0.6130, 0.6130, 0.6130,\n",
      "         0.6130],\n",
      "        [0.6140, 0.6140, 0.6140, 0.6140, 0.6140, 0.6140, 0.6140, 0.6140, 0.6140,\n",
      "         0.6140],\n",
      "        [0.6150, 0.6150, 0.6150, 0.6150, 0.6150, 0.6150, 0.6150, 0.6150, 0.6150,\n",
      "         0.6150],\n",
      "        [0.6160, 0.6160, 0.6160, 0.6160, 0.6160, 0.6160, 0.6160, 0.6160, 0.6160,\n",
      "         0.6160],\n",
      "        [0.6170, 0.6170, 0.6170, 0.6170, 0.6170, 0.6170, 0.6170, 0.6170, 0.6170,\n",
      "         0.6170],\n",
      "        [0.6180, 0.6180, 0.6180, 0.6180, 0.6180, 0.6180, 0.6180, 0.6180, 0.6180,\n",
      "         0.6180],\n",
      "        [0.6190, 0.6190, 0.6190, 0.6190, 0.6190, 0.6190, 0.6190, 0.6190, 0.6190,\n",
      "         0.6190],\n",
      "        [0.6200, 0.6200, 0.6200, 0.6200, 0.6200, 0.6200, 0.6200, 0.6200, 0.6200,\n",
      "         0.6200],\n",
      "        [0.6210, 0.6210, 0.6210, 0.6210, 0.6210, 0.6210, 0.6210, 0.6210, 0.6210,\n",
      "         0.6210],\n",
      "        [0.6220, 0.6220, 0.6220, 0.6220, 0.6220, 0.6220, 0.6220, 0.6220, 0.6220,\n",
      "         0.6220],\n",
      "        [0.6230, 0.6230, 0.6230, 0.6230, 0.6230, 0.6230, 0.6230, 0.6230, 0.6230,\n",
      "         0.6230],\n",
      "        [0.6240, 0.6240, 0.6240, 0.6240, 0.6240, 0.6240, 0.6240, 0.6240, 0.6240,\n",
      "         0.6240],\n",
      "        [0.6250, 0.6250, 0.6250, 0.6250, 0.6250, 0.6250, 0.6250, 0.6250, 0.6250,\n",
      "         0.6250],\n",
      "        [0.6260, 0.6260, 0.6260, 0.6260, 0.6260, 0.6260, 0.6260, 0.6260, 0.6260,\n",
      "         0.6260],\n",
      "        [0.6270, 0.6270, 0.6270, 0.6270, 0.6270, 0.6270, 0.6270, 0.6270, 0.6270,\n",
      "         0.6270],\n",
      "        [0.6280, 0.6280, 0.6280, 0.6280, 0.6280, 0.6280, 0.6280, 0.6280, 0.6280,\n",
      "         0.6280],\n",
      "        [0.6290, 0.6290, 0.6290, 0.6290, 0.6290, 0.6290, 0.6290, 0.6290, 0.6290,\n",
      "         0.6290]])\n",
      "tensor([0.6300])\n",
      "final_seq : 資料類型=<class 'list'>, 列數=60\n",
      "final_seq[0] : 資料類型=<class 'tuple'>, 列數=2\n",
      "final_seq[0][0] : 資料類型=<class 'torch.Tensor'>, 內容數=torch.Size([30, 10])\n",
      "final_seq[0][1] : 資料類型=<class 'torch.Tensor'>, 內容數=torch.Size([1])\n",
      "tensor([[0.7000, 0.7000, 0.7000, 0.7000, 0.7000, 0.7000, 0.7000, 0.7000, 0.7000,\n",
      "         0.7000],\n",
      "        [0.7010, 0.7010, 0.7010, 0.7010, 0.7010, 0.7010, 0.7010, 0.7010, 0.7010,\n",
      "         0.7010],\n",
      "        [0.7020, 0.7020, 0.7020, 0.7020, 0.7020, 0.7020, 0.7020, 0.7020, 0.7020,\n",
      "         0.7020],\n",
      "        [0.7030, 0.7030, 0.7030, 0.7030, 0.7030, 0.7030, 0.7030, 0.7030, 0.7030,\n",
      "         0.7030],\n",
      "        [0.7040, 0.7040, 0.7040, 0.7040, 0.7040, 0.7040, 0.7040, 0.7040, 0.7040,\n",
      "         0.7040],\n",
      "        [0.7050, 0.7050, 0.7050, 0.7050, 0.7050, 0.7050, 0.7050, 0.7050, 0.7050,\n",
      "         0.7050],\n",
      "        [0.7060, 0.7060, 0.7060, 0.7060, 0.7060, 0.7060, 0.7060, 0.7060, 0.7060,\n",
      "         0.7060],\n",
      "        [0.7070, 0.7070, 0.7070, 0.7070, 0.7070, 0.7070, 0.7070, 0.7070, 0.7070,\n",
      "         0.7070],\n",
      "        [0.7080, 0.7080, 0.7080, 0.7080, 0.7080, 0.7080, 0.7080, 0.7080, 0.7080,\n",
      "         0.7080],\n",
      "        [0.7090, 0.7090, 0.7090, 0.7090, 0.7090, 0.7090, 0.7090, 0.7090, 0.7090,\n",
      "         0.7090],\n",
      "        [0.7100, 0.7100, 0.7100, 0.7100, 0.7100, 0.7100, 0.7100, 0.7100, 0.7100,\n",
      "         0.7100],\n",
      "        [0.7110, 0.7110, 0.7110, 0.7110, 0.7110, 0.7110, 0.7110, 0.7110, 0.7110,\n",
      "         0.7110],\n",
      "        [0.7120, 0.7120, 0.7120, 0.7120, 0.7120, 0.7120, 0.7120, 0.7120, 0.7120,\n",
      "         0.7120],\n",
      "        [0.7130, 0.7130, 0.7130, 0.7130, 0.7130, 0.7130, 0.7130, 0.7130, 0.7130,\n",
      "         0.7130],\n",
      "        [0.7140, 0.7140, 0.7140, 0.7140, 0.7140, 0.7140, 0.7140, 0.7140, 0.7140,\n",
      "         0.7140],\n",
      "        [0.7150, 0.7150, 0.7150, 0.7150, 0.7150, 0.7150, 0.7150, 0.7150, 0.7150,\n",
      "         0.7150],\n",
      "        [0.7160, 0.7160, 0.7160, 0.7160, 0.7160, 0.7160, 0.7160, 0.7160, 0.7160,\n",
      "         0.7160],\n",
      "        [0.7170, 0.7170, 0.7170, 0.7170, 0.7170, 0.7170, 0.7170, 0.7170, 0.7170,\n",
      "         0.7170],\n",
      "        [0.7180, 0.7180, 0.7180, 0.7180, 0.7180, 0.7180, 0.7180, 0.7180, 0.7180,\n",
      "         0.7180],\n",
      "        [0.7190, 0.7190, 0.7190, 0.7190, 0.7190, 0.7190, 0.7190, 0.7190, 0.7190,\n",
      "         0.7190],\n",
      "        [0.7200, 0.7200, 0.7200, 0.7200, 0.7200, 0.7200, 0.7200, 0.7200, 0.7200,\n",
      "         0.7200],\n",
      "        [0.7210, 0.7210, 0.7210, 0.7210, 0.7210, 0.7210, 0.7210, 0.7210, 0.7210,\n",
      "         0.7210],\n",
      "        [0.7220, 0.7220, 0.7220, 0.7220, 0.7220, 0.7220, 0.7220, 0.7220, 0.7220,\n",
      "         0.7220],\n",
      "        [0.7230, 0.7230, 0.7230, 0.7230, 0.7230, 0.7230, 0.7230, 0.7230, 0.7230,\n",
      "         0.7230],\n",
      "        [0.7240, 0.7240, 0.7240, 0.7240, 0.7240, 0.7240, 0.7240, 0.7240, 0.7240,\n",
      "         0.7240],\n",
      "        [0.7250, 0.7250, 0.7250, 0.7250, 0.7250, 0.7250, 0.7250, 0.7250, 0.7250,\n",
      "         0.7250],\n",
      "        [0.7260, 0.7260, 0.7260, 0.7260, 0.7260, 0.7260, 0.7260, 0.7260, 0.7260,\n",
      "         0.7260],\n",
      "        [0.7270, 0.7270, 0.7270, 0.7270, 0.7270, 0.7270, 0.7270, 0.7270, 0.7270,\n",
      "         0.7270],\n",
      "        [0.7280, 0.7280, 0.7280, 0.7280, 0.7280, 0.7280, 0.7280, 0.7280, 0.7280,\n",
      "         0.7280],\n",
      "        [0.7290, 0.7290, 0.7290, 0.7290, 0.7290, 0.7290, 0.7290, 0.7290, 0.7290,\n",
      "         0.7290]])\n",
      "tensor([0.7300])\n"
     ]
    }
   ],
   "source": [
    "# 生成訓練資料\n",
    "train_data = data_generated(0, 970, 1000)\n",
    "\n",
    "# 生成驗證資料\n",
    "valid_data = data_generated(600, 60, 1000)\n",
    "\n",
    "# 生成測試資料\n",
    "test_data = data_generated(700, 60, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 生成訓練資料\n",
    "# train_data = []\n",
    "# for i in range(1000): \n",
    "#     sequence = torch.arange(i, i+6).float() # 輸入等差數列\n",
    "#     target = torch.tensor([sequence[-1] + 1])  # 預測下一個值，等於最後一個+1\n",
    "#     sequence = sequence.unsqueeze(0)\n",
    "#     sequence = torch.cat([sequence, sequence], dim=0)\n",
    "#     train_data.append((sequence, target))\n",
    "\n",
    "# # 生成驗證資料\n",
    "# valid_data = []\n",
    "# for i in range(1000,1100):\n",
    "#     sequence = torch.arange(i, i+6).float()  # 輸入等差數列\n",
    "#     target = torch.tensor([sequence[-1] + 1])  # 預測下一個值，等於最後一個+1\n",
    "#     sequence = sequence.unsqueeze(0)\n",
    "#     sequence = torch.cat([sequence, sequence], dim=0)\n",
    "#     valid_data.append((sequence, target))\n",
    "\n",
    "# # 生成測試資料\n",
    "# test_data = []\n",
    "# for i in range(1100,1200):\n",
    "#     sequence = torch.arange(i, i+6).float()  # 輸入等差數列\n",
    "#     target = torch.tensor([sequence[-1] + 1])  # 預測下一個值，等於最後一個+1\n",
    "#     sequence = sequence.unsqueeze(0)\n",
    "#     sequence = torch.cat([sequence, sequence], dim=0)\n",
    "#     test_data.append((sequence, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_seq = train_data\n",
    "# print('final_seq : 資料類型={}, 列數={}'.format(type(final_seq), len(final_seq)))\n",
    "# print('final_seq[0] : 資料類型={}, 列數={}'.format(type(final_seq[0]), len(final_seq[0])))\n",
    "# print('final_seq[0][0] : 資料類型={}, 內容數={}'.format(type(final_seq[0][0]), final_seq[0][0].shape))\n",
    "# print('final_seq[0][1] : 資料類型={}, 內容數={}'.format(type(final_seq[0][1]), final_seq[0][1].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val_loss(model, val_data, loss_function):\n",
    "    \"\"\"\n",
    "    Computes the average validation loss for a given model and validation data.\n",
    "\n",
    "    Args:\n",
    "        model: The model for which to compute the validation loss.\n",
    "        val_data: The validation data (a DataLoader object).\n",
    "        loss_function: The loss function to compute the loss.\n",
    "\n",
    "    Returns:\n",
    "        The average validation loss.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    with torch.no_grad():\n",
    "        for seq, label in val_data:\n",
    "            try:\n",
    "                seq, label = seq.to(device), label.to(device)\n",
    "                y_pred = model(seq)\n",
    "                loss = loss_function(y_pred[0].view(-1), label)\n",
    "                val_loss.append(loss.item())\n",
    "            except:\n",
    "                seq, label = seq.to(device), label.to(device)\n",
    "                print(seq.shape)\n",
    "                print(seq.dtype)\n",
    "                print(seq.type())\n",
    "                y_pred = model(seq, seq)\n",
    "                loss = loss_function(y_pred[0].view(-1), label)\n",
    "                val_loss.append(loss.item())\n",
    "\n",
    "    return np.mean(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([30, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 000 train_loss 0.32383925 val_loss 0.18474098\n",
      "epoch 001 train_loss 0.15091700 val_loss 0.09372592\n",
      "epoch 002 train_loss 0.06170616 val_loss 0.13098082\n",
      "epoch 003 train_loss 0.13024265 val_loss 0.08055688\n",
      "epoch 004 train_loss 0.11966361 val_loss 0.14404970\n",
      "epoch 005 train_loss 0.16332235 val_loss 0.24615234\n",
      "epoch 006 train_loss 0.24405560 val_loss 0.09476785\n",
      "epoch 007 train_loss 0.22396892 val_loss 0.02668462\n",
      "epoch 008 train_loss 0.18927456 val_loss 0.00248148\n",
      "epoch 009 train_loss 0.15285196 val_loss 0.00065729\n",
      "epoch 010 train_loss 0.14129237 val_loss 0.00031147\n",
      "epoch 011 train_loss 0.13686903 val_loss 0.00032003\n",
      "epoch 012 train_loss 0.13324750 val_loss 0.00050985\n",
      "epoch 013 train_loss 0.12909987 val_loss 0.00072989\n",
      "epoch 014 train_loss 0.12629376 val_loss 0.00123358\n",
      "epoch 015 train_loss 0.12544250 val_loss 0.00190325\n",
      "epoch 016 train_loss 0.12200234 val_loss 0.00255513\n",
      "epoch 017 train_loss 0.11736760 val_loss 0.00304482\n",
      "epoch 018 train_loss 0.11579930 val_loss 0.00365826\n",
      "epoch 019 train_loss 0.11522592 val_loss 0.00465628\n",
      "epoch 020 train_loss 0.11238225 val_loss 0.00540531\n",
      "epoch 021 train_loss 0.10851578 val_loss 0.00588576\n",
      "epoch 022 train_loss 0.10869483 val_loss 0.00693285\n",
      "epoch 023 train_loss 0.10579155 val_loss 0.00789891\n",
      "epoch 024 train_loss 0.10478822 val_loss 0.00894050\n",
      "epoch 025 train_loss 0.10153176 val_loss 0.00942215\n",
      "epoch 026 train_loss 0.10026250 val_loss 0.00992774\n",
      "epoch 027 train_loss 0.09880287 val_loss 0.00999798\n",
      "epoch 028 train_loss 0.09840890 val_loss nan\n",
      "epoch 029 train_loss 0.09778368 val_loss 0.00778587\n",
      "epoch 030 train_loss 0.09906961 val_loss 0.00574223\n",
      "epoch 031 train_loss 0.10175902 val_loss 0.00376571\n",
      "Epoch 00032: reducing learning rate of group 0 to 5.0000e-03.\n",
      "epoch 032 train_loss 0.09857973 val_loss 0.01053099\n",
      "epoch 033 train_loss 0.08966972 val_loss 0.01242434\n",
      "epoch 034 train_loss 0.08781569 val_loss 0.01375912\n",
      "epoch 035 train_loss 0.08729858 val_loss 0.01467627\n",
      "epoch 036 train_loss 0.08667514 val_loss 0.01518814\n",
      "epoch 037 train_loss 0.08649687 val_loss 0.01546650\n",
      "epoch 038 train_loss 0.08638549 val_loss 0.01563400\n",
      "epoch 039 train_loss 0.08640226 val_loss 0.01568628\n",
      "epoch 040 train_loss 0.08647940 val_loss 0.01587833\n",
      "epoch 041 train_loss 0.08629984 val_loss 0.01592714\n",
      "epoch 042 train_loss 0.08618908 val_loss 0.01589230\n",
      "epoch 043 train_loss 0.08610164 val_loss 0.01595359\n",
      "epoch 044 train_loss 0.08596189 val_loss 0.01592018\n",
      "epoch 045 train_loss 0.08588302 val_loss 0.01592670\n",
      "epoch 046 train_loss 0.08579665 val_loss 0.01588761\n",
      "epoch 047 train_loss 0.08599041 val_loss 0.01599914\n",
      "epoch 048 train_loss 0.08589567 val_loss 0.01607658\n",
      "epoch 049 train_loss 0.08565657 val_loss 0.01602191\n",
      "epoch 050 train_loss 0.08582460 val_loss 0.01601119\n",
      "epoch 051 train_loss 0.08578430 val_loss 0.01594795\n",
      "epoch 052 train_loss 0.08577916 val_loss 0.01604554\n",
      "Epoch 00053: reducing learning rate of group 0 to 2.5000e-03.\n",
      "epoch 053 train_loss 0.08356958 val_loss 0.01622071\n",
      "epoch 054 train_loss 0.08366862 val_loss 0.01647695\n",
      "epoch 055 train_loss 0.08365178 val_loss 0.01664999\n",
      "epoch 056 train_loss 0.08365018 val_loss 0.01678869\n",
      "epoch 057 train_loss 0.08361503 val_loss 0.01690073\n",
      "epoch 058 train_loss 0.08352077 val_loss 0.01697578\n",
      "epoch 059 train_loss 0.08358001 val_loss 0.01705859\n",
      "epoch 060 train_loss 0.08353887 val_loss 0.01710411\n",
      "epoch 061 train_loss 0.08346841 val_loss 0.01712702\n",
      "epoch 062 train_loss 0.08354282 val_loss 0.01712307\n",
      "epoch 063 train_loss 0.08352807 val_loss 0.01711663\n",
      "epoch 064 train_loss 0.08350187 val_loss 0.01714323\n",
      "epoch 065 train_loss 0.08358026 val_loss 0.01719477\n",
      "epoch 066 train_loss 0.08358835 val_loss 0.01721596\n",
      "epoch 067 train_loss 0.08353709 val_loss 0.01722166\n",
      "epoch 068 train_loss 0.08353763 val_loss 0.01722632\n",
      "epoch 069 train_loss 0.08349369 val_loss 0.01724133\n",
      "epoch 070 train_loss 0.08355586 val_loss 0.01724147\n",
      "epoch 071 train_loss 0.08356197 val_loss 0.01721151\n",
      "epoch 072 train_loss 0.08357592 val_loss 0.01721242\n",
      "epoch 073 train_loss 0.08351821 val_loss 0.01723105\n",
      "Epoch 00074: reducing learning rate of group 0 to 1.2500e-03.\n",
      "epoch 074 train_loss 0.08252297 val_loss 0.01728994\n",
      "epoch 075 train_loss 0.08249404 val_loss 0.01735484\n",
      "epoch 076 train_loss 0.08249802 val_loss 0.01742186\n",
      "epoch 077 train_loss 0.08250641 val_loss 0.01746503\n",
      "epoch 078 train_loss 0.08245714 val_loss 0.01751887\n",
      "epoch 079 train_loss 0.08248399 val_loss 0.01757603\n",
      "epoch 080 train_loss 0.08249958 val_loss 0.01760471\n",
      "epoch 081 train_loss 0.08245651 val_loss 0.01764004\n",
      "epoch 082 train_loss 0.08249723 val_loss 0.01767765\n",
      "epoch 083 train_loss 0.08248734 val_loss 0.01769910\n",
      "epoch 084 train_loss 0.08249008 val_loss 0.01772290\n",
      "epoch 085 train_loss 0.08252244 val_loss 0.01772599\n",
      "epoch 086 train_loss 0.08247333 val_loss 0.01774335\n",
      "epoch 087 train_loss 0.08247365 val_loss 0.01777335\n",
      "epoch 088 train_loss 0.08249418 val_loss 0.01777857\n",
      "epoch 089 train_loss 0.08246578 val_loss 0.01775985\n",
      "epoch 090 train_loss 0.08251842 val_loss 0.01777806\n",
      "epoch 091 train_loss 0.08250587 val_loss 0.01782082\n",
      "epoch 092 train_loss 0.08250251 val_loss 0.01782855\n",
      "epoch 093 train_loss 0.08249037 val_loss 0.01785179\n",
      "epoch 094 train_loss 0.08248036 val_loss 0.01786312\n",
      "Epoch 00095: reducing learning rate of group 0 to 6.2500e-04.\n",
      "epoch 095 train_loss 0.08192421 val_loss 0.01788576\n",
      "epoch 096 train_loss 0.08197750 val_loss 0.01791121\n",
      "epoch 097 train_loss 0.08196145 val_loss 0.01790695\n",
      "epoch 098 train_loss 0.08195246 val_loss 0.01792290\n",
      "epoch 099 train_loss 0.08196350 val_loss 0.01795011\n",
      "epoch 100 train_loss 0.08195846 val_loss 0.01798567\n",
      "epoch 101 train_loss 0.08192976 val_loss 0.01800178\n",
      "epoch 102 train_loss 0.08194514 val_loss 0.01802168\n",
      "epoch 103 train_loss 0.08193597 val_loss 0.01803830\n",
      "epoch 104 train_loss 0.08194391 val_loss 0.01804678\n",
      "epoch 105 train_loss 0.08193885 val_loss 0.01806202\n",
      "epoch 106 train_loss 0.08195176 val_loss 0.01808271\n",
      "epoch 107 train_loss 0.08193490 val_loss 0.01808373\n",
      "epoch 108 train_loss 0.08194251 val_loss 0.01808863\n",
      "epoch 109 train_loss 0.08191327 val_loss 0.01809020\n",
      "epoch 110 train_loss 0.08195394 val_loss 0.01810049\n",
      "epoch 111 train_loss 0.08194229 val_loss 0.01808636\n",
      "epoch 112 train_loss 0.08196144 val_loss 0.01809690\n",
      "epoch 113 train_loss 0.08194211 val_loss 0.01810693\n",
      "epoch 114 train_loss 0.08193097 val_loss 0.01812076\n",
      "epoch 115 train_loss 0.08194146 val_loss 0.01813934\n",
      "Epoch 00116: reducing learning rate of group 0 to 3.1250e-04.\n",
      "epoch 116 train_loss 0.08165508 val_loss 0.01813947\n",
      "epoch 117 train_loss 0.08167335 val_loss 0.01815652\n",
      "epoch 118 train_loss 0.08169363 val_loss 0.01816909\n",
      "epoch 119 train_loss 0.08164388 val_loss 0.01817477\n",
      "epoch 120 train_loss 0.08166313 val_loss 0.01817579\n",
      "epoch 121 train_loss 0.08165888 val_loss 0.01817548\n",
      "epoch 122 train_loss 0.08168130 val_loss 0.01818637\n",
      "epoch 123 train_loss 0.08166039 val_loss 0.01819693\n",
      "epoch 124 train_loss 0.08165768 val_loss 0.01820203\n",
      "epoch 125 train_loss 0.08166982 val_loss 0.01820402\n",
      "epoch 126 train_loss 0.08165511 val_loss 0.01821342\n",
      "epoch 127 train_loss 0.08164838 val_loss 0.01821079\n",
      "epoch 128 train_loss 0.08167146 val_loss 0.01822437\n",
      "epoch 129 train_loss 0.08169805 val_loss 0.01821540\n",
      "epoch 130 train_loss 0.08169318 val_loss 0.01822149\n",
      "epoch 131 train_loss 0.08167599 val_loss 0.01822722\n",
      "epoch 132 train_loss 0.08165736 val_loss 0.01823568\n",
      "epoch 133 train_loss 0.08170473 val_loss 0.01824253\n",
      "epoch 134 train_loss 0.08165264 val_loss 0.01825453\n",
      "epoch 135 train_loss 0.08166592 val_loss 0.01826598\n",
      "epoch 136 train_loss 0.08166977 val_loss 0.01826763\n",
      "Epoch 00137: reducing learning rate of group 0 to 1.5625e-04.\n",
      "epoch 137 train_loss 0.08153237 val_loss 0.01826867\n",
      "epoch 138 train_loss 0.08152496 val_loss 0.01826822\n",
      "epoch 139 train_loss 0.08151358 val_loss 0.01827564\n",
      "epoch 140 train_loss 0.08153383 val_loss 0.01827848\n",
      "epoch 141 train_loss 0.08152346 val_loss 0.01828257\n",
      "epoch 142 train_loss 0.08151980 val_loss 0.01828256\n",
      "epoch 143 train_loss 0.08152452 val_loss 0.01829105\n",
      "epoch 144 train_loss 0.08149945 val_loss 0.01830154\n",
      "epoch 145 train_loss 0.08153145 val_loss 0.01830563\n",
      "epoch 146 train_loss 0.08152247 val_loss 0.01831043\n",
      "epoch 147 train_loss 0.08154050 val_loss 0.01831544\n",
      "epoch 148 train_loss 0.08152067 val_loss 0.01831614\n",
      "epoch 149 train_loss 0.08152094 val_loss 0.01832104\n",
      "epoch 150 train_loss 0.08153908 val_loss 0.01832674\n",
      "epoch 151 train_loss 0.08151951 val_loss 0.01833067\n",
      "epoch 152 train_loss 0.08152978 val_loss 0.01833392\n",
      "epoch 153 train_loss 0.08151949 val_loss 0.01833580\n",
      "epoch 154 train_loss 0.08151558 val_loss 0.01834611\n",
      "epoch 155 train_loss 0.08151959 val_loss 0.01835149\n",
      "epoch 156 train_loss 0.08152361 val_loss 0.01834970\n",
      "epoch 157 train_loss 0.08155520 val_loss 0.01835418\n",
      "Epoch 00158: reducing learning rate of group 0 to 7.8125e-05.\n",
      "epoch 158 train_loss 0.08148213 val_loss 0.01835298\n",
      "epoch 159 train_loss 0.08142227 val_loss 0.01835353\n",
      "epoch 160 train_loss 0.08146239 val_loss 0.01835961\n",
      "epoch 161 train_loss 0.08149847 val_loss 0.01836200\n",
      "epoch 162 train_loss 0.08148279 val_loss 0.01836285\n",
      "epoch 163 train_loss 0.08146524 val_loss 0.01835860\n",
      "epoch 164 train_loss 0.08146369 val_loss 0.01835743\n",
      "epoch 165 train_loss 0.08146416 val_loss 0.01835883\n",
      "epoch 166 train_loss 0.08146583 val_loss 0.01836164\n",
      "epoch 167 train_loss 0.08145201 val_loss 0.01836302\n",
      "epoch 168 train_loss 0.08144441 val_loss 0.01836339\n",
      "epoch 169 train_loss 0.08145912 val_loss 0.01836496\n",
      "epoch 170 train_loss 0.08145503 val_loss 0.01836847\n",
      "epoch 171 train_loss 0.08144767 val_loss 0.01836814\n",
      "epoch 172 train_loss 0.08145653 val_loss 0.01836977\n",
      "epoch 173 train_loss 0.08142665 val_loss 0.01837703\n",
      "epoch 174 train_loss 0.08145829 val_loss 0.01837780\n",
      "epoch 175 train_loss 0.08147150 val_loss 0.01837709\n",
      "epoch 176 train_loss 0.08146276 val_loss 0.01837874\n",
      "epoch 177 train_loss 0.08147780 val_loss 0.01837711\n",
      "epoch 178 train_loss 0.08144591 val_loss 0.01837551\n",
      "Epoch 00179: reducing learning rate of group 0 to 3.9063e-05.\n",
      "epoch 179 train_loss 0.08144666 val_loss 0.01837690\n",
      "epoch 180 train_loss 0.08142974 val_loss 0.01837462\n",
      "epoch 181 train_loss 0.08146056 val_loss 0.01837384\n",
      "epoch 182 train_loss 0.08140207 val_loss 0.01837354\n",
      "epoch 183 train_loss 0.08143039 val_loss 0.01837342\n",
      "epoch 184 train_loss 0.08144301 val_loss 0.01837342\n",
      "epoch 185 train_loss 0.08140778 val_loss 0.01837248\n",
      "epoch 186 train_loss 0.08140253 val_loss 0.01837299\n",
      "epoch 187 train_loss 0.08143364 val_loss 0.01837462\n",
      "epoch 188 train_loss 0.08141244 val_loss 0.01837607\n",
      "epoch 189 train_loss 0.08143119 val_loss 0.01837706\n",
      "epoch 190 train_loss 0.08142621 val_loss 0.01837780\n",
      "epoch 191 train_loss 0.08142890 val_loss 0.01837745\n",
      "epoch 192 train_loss 0.08140640 val_loss 0.01837964\n",
      "epoch 193 train_loss 0.08142320 val_loss 0.01838056\n",
      "epoch 194 train_loss 0.08143629 val_loss 0.01837919\n",
      "epoch 195 train_loss 0.08140767 val_loss 0.01837886\n",
      "epoch 196 train_loss 0.08141643 val_loss 0.01837636\n",
      "epoch 197 train_loss 0.08143680 val_loss 0.01837656\n",
      "epoch 198 train_loss 0.08140554 val_loss 0.01837549\n",
      "epoch 199 train_loss 0.08141780 val_loss 0.01837858\n",
      "Epoch 00200: reducing learning rate of group 0 to 1.9531e-05.\n",
      "epoch 200 train_loss 0.08139219 val_loss 0.01837889\n",
      "epoch 201 train_loss 0.08141553 val_loss 0.01837860\n",
      "epoch 202 train_loss 0.08141405 val_loss 0.01837964\n",
      "epoch 203 train_loss 0.08139210 val_loss 0.01838076\n",
      "epoch 204 train_loss 0.08140326 val_loss 0.01838133\n",
      "epoch 205 train_loss 0.08142156 val_loss 0.01838044\n",
      "epoch 206 train_loss 0.08140496 val_loss 0.01838020\n",
      "epoch 207 train_loss 0.08144963 val_loss 0.01837900\n",
      "epoch 208 train_loss 0.08138370 val_loss 0.01837934\n",
      "epoch 209 train_loss 0.08141472 val_loss 0.01838059\n",
      "epoch 210 train_loss 0.08141578 val_loss 0.01838062\n",
      "epoch 211 train_loss 0.08141880 val_loss 0.01838204\n",
      "epoch 212 train_loss 0.08141123 val_loss 0.01838405\n",
      "epoch 213 train_loss 0.08139639 val_loss 0.01838615\n",
      "epoch 214 train_loss 0.08140093 val_loss 0.01838673\n",
      "epoch 215 train_loss 0.08141068 val_loss 0.01838602\n",
      "epoch 216 train_loss 0.08139328 val_loss 0.01838623\n",
      "epoch 217 train_loss 0.08138806 val_loss 0.01838711\n",
      "epoch 218 train_loss 0.08139579 val_loss 0.01838722\n",
      "epoch 219 train_loss 0.08138666 val_loss 0.01838839\n",
      "epoch 220 train_loss 0.08140837 val_loss 0.01838844\n",
      "Epoch 00221: reducing learning rate of group 0 to 9.7656e-06.\n",
      "epoch 221 train_loss 0.08141414 val_loss 0.01838830\n",
      "epoch 222 train_loss 0.08140250 val_loss 0.01838777\n",
      "epoch 223 train_loss 0.08138594 val_loss 0.01838703\n",
      "epoch 224 train_loss 0.08137221 val_loss 0.01838703\n",
      "epoch 225 train_loss 0.08135925 val_loss 0.01838867\n",
      "epoch 226 train_loss 0.08140006 val_loss 0.01838875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 227 train_loss 0.08139424 val_loss 0.01838908\n"
     ]
    }
   ],
   "source": [
    "# 模型訓練\n",
    "\n",
    "#for epoch in tqdm(range(1000)):\n",
    "for epoch in range(10000):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    time = 0\n",
    "    for sequence, target in train_data:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        sequence = sequence.to(device)\n",
    "        # print(sequence.shape)\n",
    "        # print(sequence.dtype)\n",
    "        # print(sequence.type())\n",
    "        target = target.to(device)\n",
    "        # print(target.shape)\n",
    "        # print(target.dtype)\n",
    "        # print(target.type())\n",
    "        output = model(sequence)\n",
    "        #print('model :', output.view(-1)[0])\n",
    "        #print('true :', target.view(-1)[0])\n",
    "        \n",
    "        # 計算損失\n",
    "        loss = criterion(output.view(-1), target.view(-1)).to(device)\n",
    "\n",
    "        # 反向傳播與參數更新\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        time += 1\n",
    "    #print('結束1')  \n",
    "    val_loss = get_val_loss(model, valid_data, criterion)\n",
    "    #print('結束2')  \n",
    "    print('epoch {:03d} train_loss {:.8f} val_loss {:.8f}'.format(epoch, total_loss/time, val_loss))\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "\n",
    "# 模型測試\n",
    "model.eval()\n",
    "correct_predictions = 0\n",
    "ans = 730\n",
    "with torch.no_grad():\n",
    "    for sequence, target in test_data:\n",
    "        sequence = sequence.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(sequence)\n",
    "        for i in output:\n",
    "            print('預測 :', i, '正解 :', ans)\n",
    "            ans += 1\n",
    "        loss = criterion(output, target).to(device)\n",
    "    \n",
    "        predicted_value = output.view(-1).item()\n",
    "       \n",
    "\n",
    "accuracy = correct_predictions / len(test_data)\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
