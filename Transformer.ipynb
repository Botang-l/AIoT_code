{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義 Transforme rModel\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, num_heads):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, 1, batch_first=True)\n",
    "        self.transformer = Transformer(\n",
    "            d_model=hidden_dim,\n",
    "            #d_model=input_dim,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        #self.fc = nn.Linear(BATCH_SIZE*input_dim, output_dim)\n",
    "\n",
    "    def forward(self, src):\n",
    "    \n",
    "        # batch_size, _ = src.shape[0], src.shape[1]\n",
    "        # h_0 = torch.randn(1, batch_size,  self.hidden_dim).to(device)\n",
    "        # c_0 = torch.randn(1, batch_size,  self.hidden_dim).to(device)\n",
    "        # src, _ = self.lstm(src, (h_0, c_0))\n",
    "        src = self.embedding(src)\n",
    "        # #print('lstm後的大小:',src.shape)\n",
    "        output = self.transformer(src, src)\n",
    "        #print('T後的大小:',output.shape)\n",
    "        output = self.fc(output)\n",
    "        #output = self.fc(output)\n",
    "        #print('最終大小:',output.shape)\n",
    "        output = output[:, -1, :]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型參數\n",
    "input_dim = 10 # 輸入詞彙表大小\n",
    "hidden_dim = 32  # 隱藏層維度\n",
    "output_dim = 1  # 輸出維度，輸出維度為1，代表預測的下一個數\n",
    "num_layers = 4  # Transformer Encoder/Decoder 層數\n",
    "num_heads = 8  # Attention heads \n",
    "\n",
    "\n",
    "\n",
    "# 創建 Transformer\n",
    "model = TransformerModel(input_dim, hidden_dim, output_dim, num_layers, num_heads)\n",
    "model.to(device)\n",
    "\n",
    "# 定義損失函數和優化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=1e-6)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(final_seq, batch_size):\n",
    "    \"\"\"\n",
    "    Batches the sequence data using PyTorch DataLoader.\n",
    "\n",
    "    Args:\n",
    "        final_seq: A list of tuples containing sequence input data and target data.\n",
    "        batch_size: The desired batch size.\n",
    "\n",
    "    Returns:\n",
    "        A DataLoader object containing the batched sequence data.\n",
    "\n",
    "    \"\"\"\n",
    "    final_seq = DataLoader(dataset=final_seq, batch_size=batch_size, shuffle=False, num_workers=0, drop_last=False)\n",
    "\n",
    "    return final_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成 l * w * h 維度的資料\n",
    "# l : batch_size\n",
    "# w : sequence length\n",
    "# h : amounts of features \n",
    "# 以下以 30 * 6 * 1 做示範\n",
    "def data_generated(beg, num):\n",
    "    final_seq = []\n",
    "    for i in range(num):\n",
    "        end = beg + 30\n",
    "        q_tensors = [torch.tensor([i]*10, dtype=torch.float32) for i in range(beg, end)]\n",
    "        q_tensors = torch.stack(q_tensors)\n",
    "        a_tensors = torch.tensor([end], dtype=torch.float32) \n",
    "        final_seq.append((q_tensors, a_tensors))\n",
    "        beg += 1\n",
    "    print('final_seq : 資料類型={}, 列數={}'.format(type(final_seq), len(final_seq)))\n",
    "    print('final_seq[0] : 資料類型={}, 列數={}'.format(type(final_seq[0]), len(final_seq[0])))\n",
    "    print('final_seq[0][0] : 資料類型={}, 內容數={}'.format(type(final_seq[0][0]), final_seq[0][0].shape))\n",
    "    print('final_seq[0][1] : 資料類型={}, 內容數={}'.format(type(final_seq[0][1]), final_seq[0][1].shape))\n",
    "    print(final_seq[0][0])\n",
    "    print(final_seq[0][1])\n",
    "    data = batch_data(final_seq, 30)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_seq : 資料類型=<class 'list'>, 列數=571\n",
      "final_seq[0] : 資料類型=<class 'tuple'>, 列數=2\n",
      "final_seq[0][0] : 資料類型=<class 'torch.Tensor'>, 內容數=torch.Size([30, 10])\n",
      "final_seq[0][1] : 資料類型=<class 'torch.Tensor'>, 內容數=torch.Size([1])\n",
      "tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [ 2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.],\n",
      "        [ 3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.],\n",
      "        [ 4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.],\n",
      "        [ 5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.],\n",
      "        [ 6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.],\n",
      "        [ 7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.],\n",
      "        [ 8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.],\n",
      "        [ 9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.],\n",
      "        [10., 10., 10., 10., 10., 10., 10., 10., 10., 10.],\n",
      "        [11., 11., 11., 11., 11., 11., 11., 11., 11., 11.],\n",
      "        [12., 12., 12., 12., 12., 12., 12., 12., 12., 12.],\n",
      "        [13., 13., 13., 13., 13., 13., 13., 13., 13., 13.],\n",
      "        [14., 14., 14., 14., 14., 14., 14., 14., 14., 14.],\n",
      "        [15., 15., 15., 15., 15., 15., 15., 15., 15., 15.],\n",
      "        [16., 16., 16., 16., 16., 16., 16., 16., 16., 16.],\n",
      "        [17., 17., 17., 17., 17., 17., 17., 17., 17., 17.],\n",
      "        [18., 18., 18., 18., 18., 18., 18., 18., 18., 18.],\n",
      "        [19., 19., 19., 19., 19., 19., 19., 19., 19., 19.],\n",
      "        [20., 20., 20., 20., 20., 20., 20., 20., 20., 20.],\n",
      "        [21., 21., 21., 21., 21., 21., 21., 21., 21., 21.],\n",
      "        [22., 22., 22., 22., 22., 22., 22., 22., 22., 22.],\n",
      "        [23., 23., 23., 23., 23., 23., 23., 23., 23., 23.],\n",
      "        [24., 24., 24., 24., 24., 24., 24., 24., 24., 24.],\n",
      "        [25., 25., 25., 25., 25., 25., 25., 25., 25., 25.],\n",
      "        [26., 26., 26., 26., 26., 26., 26., 26., 26., 26.],\n",
      "        [27., 27., 27., 27., 27., 27., 27., 27., 27., 27.],\n",
      "        [28., 28., 28., 28., 28., 28., 28., 28., 28., 28.],\n",
      "        [29., 29., 29., 29., 29., 29., 29., 29., 29., 29.]])\n",
      "tensor([30.])\n"
     ]
    }
   ],
   "source": [
    "data = data_generated(0, 571)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_seq : 資料類型=<class 'list'>, 列數=6000\n",
      "final_seq[0] : 資料類型=<class 'tuple'>, 列數=2\n",
      "final_seq[0][0] : 資料類型=<class 'torch.Tensor'>, 內容數=torch.Size([30, 10])\n",
      "final_seq[0][1] : 資料類型=<class 'torch.Tensor'>, 內容數=torch.Size([1])\n",
      "tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [ 2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.],\n",
      "        [ 3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.],\n",
      "        [ 4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.],\n",
      "        [ 5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.],\n",
      "        [ 6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.],\n",
      "        [ 7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.],\n",
      "        [ 8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.],\n",
      "        [ 9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.],\n",
      "        [10., 10., 10., 10., 10., 10., 10., 10., 10., 10.],\n",
      "        [11., 11., 11., 11., 11., 11., 11., 11., 11., 11.],\n",
      "        [12., 12., 12., 12., 12., 12., 12., 12., 12., 12.],\n",
      "        [13., 13., 13., 13., 13., 13., 13., 13., 13., 13.],\n",
      "        [14., 14., 14., 14., 14., 14., 14., 14., 14., 14.],\n",
      "        [15., 15., 15., 15., 15., 15., 15., 15., 15., 15.],\n",
      "        [16., 16., 16., 16., 16., 16., 16., 16., 16., 16.],\n",
      "        [17., 17., 17., 17., 17., 17., 17., 17., 17., 17.],\n",
      "        [18., 18., 18., 18., 18., 18., 18., 18., 18., 18.],\n",
      "        [19., 19., 19., 19., 19., 19., 19., 19., 19., 19.],\n",
      "        [20., 20., 20., 20., 20., 20., 20., 20., 20., 20.],\n",
      "        [21., 21., 21., 21., 21., 21., 21., 21., 21., 21.],\n",
      "        [22., 22., 22., 22., 22., 22., 22., 22., 22., 22.],\n",
      "        [23., 23., 23., 23., 23., 23., 23., 23., 23., 23.],\n",
      "        [24., 24., 24., 24., 24., 24., 24., 24., 24., 24.],\n",
      "        [25., 25., 25., 25., 25., 25., 25., 25., 25., 25.],\n",
      "        [26., 26., 26., 26., 26., 26., 26., 26., 26., 26.],\n",
      "        [27., 27., 27., 27., 27., 27., 27., 27., 27., 27.],\n",
      "        [28., 28., 28., 28., 28., 28., 28., 28., 28., 28.],\n",
      "        [29., 29., 29., 29., 29., 29., 29., 29., 29., 29.]])\n",
      "tensor([30.])\n",
      "final_seq : 資料類型=<class 'list'>, 列數=60\n",
      "final_seq[0] : 資料類型=<class 'tuple'>, 列數=2\n",
      "final_seq[0][0] : 資料類型=<class 'torch.Tensor'>, 內容數=torch.Size([30, 10])\n",
      "final_seq[0][1] : 資料類型=<class 'torch.Tensor'>, 內容數=torch.Size([1])\n",
      "tensor([[600., 600., 600., 600., 600., 600., 600., 600., 600., 600.],\n",
      "        [601., 601., 601., 601., 601., 601., 601., 601., 601., 601.],\n",
      "        [602., 602., 602., 602., 602., 602., 602., 602., 602., 602.],\n",
      "        [603., 603., 603., 603., 603., 603., 603., 603., 603., 603.],\n",
      "        [604., 604., 604., 604., 604., 604., 604., 604., 604., 604.],\n",
      "        [605., 605., 605., 605., 605., 605., 605., 605., 605., 605.],\n",
      "        [606., 606., 606., 606., 606., 606., 606., 606., 606., 606.],\n",
      "        [607., 607., 607., 607., 607., 607., 607., 607., 607., 607.],\n",
      "        [608., 608., 608., 608., 608., 608., 608., 608., 608., 608.],\n",
      "        [609., 609., 609., 609., 609., 609., 609., 609., 609., 609.],\n",
      "        [610., 610., 610., 610., 610., 610., 610., 610., 610., 610.],\n",
      "        [611., 611., 611., 611., 611., 611., 611., 611., 611., 611.],\n",
      "        [612., 612., 612., 612., 612., 612., 612., 612., 612., 612.],\n",
      "        [613., 613., 613., 613., 613., 613., 613., 613., 613., 613.],\n",
      "        [614., 614., 614., 614., 614., 614., 614., 614., 614., 614.],\n",
      "        [615., 615., 615., 615., 615., 615., 615., 615., 615., 615.],\n",
      "        [616., 616., 616., 616., 616., 616., 616., 616., 616., 616.],\n",
      "        [617., 617., 617., 617., 617., 617., 617., 617., 617., 617.],\n",
      "        [618., 618., 618., 618., 618., 618., 618., 618., 618., 618.],\n",
      "        [619., 619., 619., 619., 619., 619., 619., 619., 619., 619.],\n",
      "        [620., 620., 620., 620., 620., 620., 620., 620., 620., 620.],\n",
      "        [621., 621., 621., 621., 621., 621., 621., 621., 621., 621.],\n",
      "        [622., 622., 622., 622., 622., 622., 622., 622., 622., 622.],\n",
      "        [623., 623., 623., 623., 623., 623., 623., 623., 623., 623.],\n",
      "        [624., 624., 624., 624., 624., 624., 624., 624., 624., 624.],\n",
      "        [625., 625., 625., 625., 625., 625., 625., 625., 625., 625.],\n",
      "        [626., 626., 626., 626., 626., 626., 626., 626., 626., 626.],\n",
      "        [627., 627., 627., 627., 627., 627., 627., 627., 627., 627.],\n",
      "        [628., 628., 628., 628., 628., 628., 628., 628., 628., 628.],\n",
      "        [629., 629., 629., 629., 629., 629., 629., 629., 629., 629.]])\n",
      "tensor([630.])\n",
      "final_seq : 資料類型=<class 'list'>, 列數=60\n",
      "final_seq[0] : 資料類型=<class 'tuple'>, 列數=2\n",
      "final_seq[0][0] : 資料類型=<class 'torch.Tensor'>, 內容數=torch.Size([30, 10])\n",
      "final_seq[0][1] : 資料類型=<class 'torch.Tensor'>, 內容數=torch.Size([1])\n",
      "tensor([[700., 700., 700., 700., 700., 700., 700., 700., 700., 700.],\n",
      "        [701., 701., 701., 701., 701., 701., 701., 701., 701., 701.],\n",
      "        [702., 702., 702., 702., 702., 702., 702., 702., 702., 702.],\n",
      "        [703., 703., 703., 703., 703., 703., 703., 703., 703., 703.],\n",
      "        [704., 704., 704., 704., 704., 704., 704., 704., 704., 704.],\n",
      "        [705., 705., 705., 705., 705., 705., 705., 705., 705., 705.],\n",
      "        [706., 706., 706., 706., 706., 706., 706., 706., 706., 706.],\n",
      "        [707., 707., 707., 707., 707., 707., 707., 707., 707., 707.],\n",
      "        [708., 708., 708., 708., 708., 708., 708., 708., 708., 708.],\n",
      "        [709., 709., 709., 709., 709., 709., 709., 709., 709., 709.],\n",
      "        [710., 710., 710., 710., 710., 710., 710., 710., 710., 710.],\n",
      "        [711., 711., 711., 711., 711., 711., 711., 711., 711., 711.],\n",
      "        [712., 712., 712., 712., 712., 712., 712., 712., 712., 712.],\n",
      "        [713., 713., 713., 713., 713., 713., 713., 713., 713., 713.],\n",
      "        [714., 714., 714., 714., 714., 714., 714., 714., 714., 714.],\n",
      "        [715., 715., 715., 715., 715., 715., 715., 715., 715., 715.],\n",
      "        [716., 716., 716., 716., 716., 716., 716., 716., 716., 716.],\n",
      "        [717., 717., 717., 717., 717., 717., 717., 717., 717., 717.],\n",
      "        [718., 718., 718., 718., 718., 718., 718., 718., 718., 718.],\n",
      "        [719., 719., 719., 719., 719., 719., 719., 719., 719., 719.],\n",
      "        [720., 720., 720., 720., 720., 720., 720., 720., 720., 720.],\n",
      "        [721., 721., 721., 721., 721., 721., 721., 721., 721., 721.],\n",
      "        [722., 722., 722., 722., 722., 722., 722., 722., 722., 722.],\n",
      "        [723., 723., 723., 723., 723., 723., 723., 723., 723., 723.],\n",
      "        [724., 724., 724., 724., 724., 724., 724., 724., 724., 724.],\n",
      "        [725., 725., 725., 725., 725., 725., 725., 725., 725., 725.],\n",
      "        [726., 726., 726., 726., 726., 726., 726., 726., 726., 726.],\n",
      "        [727., 727., 727., 727., 727., 727., 727., 727., 727., 727.],\n",
      "        [728., 728., 728., 728., 728., 728., 728., 728., 728., 728.],\n",
      "        [729., 729., 729., 729., 729., 729., 729., 729., 729., 729.]])\n",
      "tensor([730.])\n"
     ]
    }
   ],
   "source": [
    "# 生成訓練資料\n",
    "train_data = data_generated(0, 6000)\n",
    "\n",
    "# 生成驗證資料\n",
    "valid_data = data_generated(600, 60)\n",
    "\n",
    "# 生成測試資料\n",
    "test_data = data_generated(700, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 生成訓練資料\n",
    "# train_data = []\n",
    "# for i in range(1000): \n",
    "#     sequence = torch.arange(i, i+6).float() # 輸入等差數列\n",
    "#     target = torch.tensor([sequence[-1] + 1])  # 預測下一個值，等於最後一個+1\n",
    "#     sequence = sequence.unsqueeze(0)\n",
    "#     sequence = torch.cat([sequence, sequence], dim=0)\n",
    "#     train_data.append((sequence, target))\n",
    "\n",
    "# # 生成驗證資料\n",
    "# valid_data = []\n",
    "# for i in range(1000,1100):\n",
    "#     sequence = torch.arange(i, i+6).float()  # 輸入等差數列\n",
    "#     target = torch.tensor([sequence[-1] + 1])  # 預測下一個值，等於最後一個+1\n",
    "#     sequence = sequence.unsqueeze(0)\n",
    "#     sequence = torch.cat([sequence, sequence], dim=0)\n",
    "#     valid_data.append((sequence, target))\n",
    "\n",
    "# # 生成測試資料\n",
    "# test_data = []\n",
    "# for i in range(1100,1200):\n",
    "#     sequence = torch.arange(i, i+6).float()  # 輸入等差數列\n",
    "#     target = torch.tensor([sequence[-1] + 1])  # 預測下一個值，等於最後一個+1\n",
    "#     sequence = sequence.unsqueeze(0)\n",
    "#     sequence = torch.cat([sequence, sequence], dim=0)\n",
    "#     test_data.append((sequence, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_seq = train_data\n",
    "# print('final_seq : 資料類型={}, 列數={}'.format(type(final_seq), len(final_seq)))\n",
    "# print('final_seq[0] : 資料類型={}, 列數={}'.format(type(final_seq[0]), len(final_seq[0])))\n",
    "# print('final_seq[0][0] : 資料類型={}, 內容數={}'.format(type(final_seq[0][0]), final_seq[0][0].shape))\n",
    "# print('final_seq[0][1] : 資料類型={}, 內容數={}'.format(type(final_seq[0][1]), final_seq[0][1].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val_loss(model, val_data, loss_function):\n",
    "    \"\"\"\n",
    "    Computes the average validation loss for a given model and validation data.\n",
    "\n",
    "    Args:\n",
    "        model: The model for which to compute the validation loss.\n",
    "        val_data: The validation data (a DataLoader object).\n",
    "        loss_function: The loss function to compute the loss.\n",
    "\n",
    "    Returns:\n",
    "        The average validation loss.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    with torch.no_grad():\n",
    "        for seq, label in val_data:\n",
    "            try:\n",
    "                seq, label = seq.to(device), label.to(device)\n",
    "                y_pred = model(seq)\n",
    "                loss = loss_function(y_pred[0].view(-1), label)\n",
    "                val_loss.append(loss.item())\n",
    "            except:\n",
    "                seq, label = seq.to(device), label.to(device)\n",
    "                print(seq.shape)\n",
    "                print(seq.dtype)\n",
    "                print(seq.type())\n",
    "                y_pred = model(seq, seq)\n",
    "                loss = loss_function(y_pred[0].view(-1), label)\n",
    "                val_loss.append(loss.item())\n",
    "\n",
    "    return np.mean(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([30, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 000 train_loss 9890255.43455444 val_loss 32619.92382812\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m loss \u001b[39m=\u001b[39m criterion(output\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), target\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     26\u001b[0m \u001b[39m# 反向傳播與參數更新\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     28\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     30\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 模型訓練\n",
    "\n",
    "#for epoch in tqdm(range(1000)):\n",
    "for epoch in range(10000):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    time = 0\n",
    "    for sequence, target in train_data:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        sequence = sequence.to(device)\n",
    "        # print(sequence.shape)\n",
    "        # print(sequence.dtype)\n",
    "        # print(sequence.type())\n",
    "        target = target.to(device)\n",
    "        # print(target.shape)\n",
    "        # print(target.dtype)\n",
    "        # print(target.type())\n",
    "        output = model(sequence)\n",
    "        #print('model :', output.view(-1)[0])\n",
    "        #print('true :', target.view(-1)[0])\n",
    "        \n",
    "        # 計算損失\n",
    "        loss = criterion(output.view(-1), target.view(-1)).to(device)\n",
    "\n",
    "        # 反向傳播與參數更新\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        time += 1\n",
    "    #print('結束1')  \n",
    "    val_loss = get_val_loss(model, valid_data, criterion)\n",
    "    #print('結束2')  \n",
    "    print('epoch {:03d} train_loss {:.8f} val_loss {:.8f}'.format(epoch, total_loss/time, val_loss))\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "\n",
    "# 模型測試\n",
    "model.eval()\n",
    "correct_predictions = 0\n",
    "ans = 730\n",
    "with torch.no_grad():\n",
    "    for sequence, target in test_data:\n",
    "        sequence = sequence.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(sequence)\n",
    "        for i in output:\n",
    "            print('預測 :', i, '正解 :', ans)\n",
    "            ans += 1\n",
    "        loss = criterion(output, target).to(device)\n",
    "    \n",
    "        predicted_value = output.view(-1).item()\n",
    "       \n",
    "\n",
    "accuracy = correct_predictions / len(test_data)\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
