{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from Memory import *\n",
    "from Model import *\n",
    "from Environment import *\n",
    "from Strategy import *\n",
    "from util import *\n",
    "from args import *\n",
    "from Environment import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = torch.load('model/policy_net.pth')\n",
    "target_net = torch.load('model/target_net.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最終決策 policy net:\n",
      "----------\n",
      "tensor([[0.2876, 0.4556]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6095, 0.8062]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9314, 1.1568]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[1.2533, 1.5073]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[1.5753, 1.8579]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Total Reward : 0.8\n",
      "[1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'i_episode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 28\u001b[0m\n\u001b[1;32m     24\u001b[0m env\u001b[39m.\u001b[39mdisplayTotalReward() \n\u001b[1;32m     25\u001b[0m \u001b[39mprint\u001b[39m(env\u001b[39m.\u001b[39mactionlist)\n\u001b[0;32m---> 28\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m最終決策 target net:\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(i_episode\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m     29\u001b[0m \u001b[39m# Initialize the environment and get it's state\u001b[39;00m\n\u001b[1;32m     30\u001b[0m state, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'i_episode' is not defined"
     ]
    }
   ],
   "source": [
    "print('最終決策 policy net:')\n",
    "# Initialize the environment and get it's state\n",
    "state, _ = env.reset()\n",
    "#env.displayPosition()\n",
    "print('-'*10)\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "for t in count():\n",
    "    if(t > 100):\n",
    "        break\n",
    "    action = policy_net(state).max(1)[1].view(1, 1)\n",
    "    print(policy_net(state))\n",
    "    observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "    reward = torch.tensor([reward], device=device)\n",
    "    done = terminated or truncated\n",
    "    if terminated:\n",
    "        state = None\n",
    "    else:\n",
    "        state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            total_reward.append(env.TotalReward())\n",
    "            #plot_durations(total_reward)\n",
    "            break     \n",
    "env.displayTotalReward() \n",
    "print(env.actionlist)\n",
    "\n",
    "\n",
    "print('最終決策 target net:')\n",
    "# Initialize the environment and get it's state\n",
    "state, _ = env.reset()\n",
    "#env.displayPosition()\n",
    "print('-'*10)\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "for t in count():\n",
    "    if(t > 100):\n",
    "        break\n",
    "    action = target_net(state).max(1)[1].view(1, 1)\n",
    "    print(target_net(state))\n",
    "    observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "    reward = torch.tensor([reward], device=device)\n",
    "    done = terminated or truncated\n",
    "    if terminated:\n",
    "        state = None\n",
    "    else:\n",
    "        state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            total_reward.append(env.TotalReward())\n",
    "            #plot_durations(total_reward)\n",
    "            break\n",
    "env.displayTotalReward()\n",
    "print(env.actionlist)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
